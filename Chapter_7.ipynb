{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7\n",
    "\n",
    "## ch7.1\n",
    "### CNN 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `CNN`은 `컨볼루션 계층`과 `풀링 계층`으로 구성된다.\n",
    "\n",
    "- 컨볼루션과 풀링 계층은 2차원의 평면 행렬에서 지정한 영역의 값들을 하나의 값으로 압축하는 역할을 한다.\n",
    "\n",
    "- 단, 압축시에 `컨볼루션 계층`은 가중치와 편향 적용하고, `풀링 계층`은 값들 중 하나를 선택해서 가져옴\n",
    "\n",
    "- 컨볼루션 계층에서 윈도우 크기만큼의 가중치와 1개의 편향을 적용한다. 윈도우의 크기가 `3x3`이면 `3x3`의 가중체과 `1`개의 편향이 필요하다. 그리고 그 커널은 모든 윈도우에 공통으로 적용된다.\n",
    "\n",
    "- 입력층이 28x28 개라고 했을 때 기본신경망은 784개의 가중치가 필요하지만, 컨볼루션 계층에서는 9개의 가중치만 찾으면 되므로, 계산량이 매우 적어져 학습이 빠르고 효울적이다.\n",
    "\n",
    "- 보통 커널을 여러개 사용하며, 커널의 개수는 하이퍼 파라미터이다.\n",
    "\n",
    "`커널`or`필터` : 위의 예제에서 `3x3`개의 가중치와 `1`개의 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ch7.2\n",
    "### 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-4f41eabc6613>:6: read_data_sets (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as: tensorflow_datasets.load('mnist')\n",
      "WARNING:tensorflow:From c:\\python3.7\\lib\\site-packages\\tensorflow_core\\examples\\tutorials\\mnist\\input_data.py:297: _maybe_download (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\python3.7\\lib\\site-packages\\tensorflow_core\\examples\\tutorials\\mnist\\input_data.py:299: _extract_images (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python3.7\\lib\\site-packages\\tensorflow_core\\examples\\tutorials\\mnist\\input_data.py:304: _extract_labels (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python3.7\\lib\\site-packages\\tensorflow_core\\examples\\tutorials\\mnist\\input_data.py:112: _dense_to_one_hot (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python3.7\\lib\\site-packages\\tensorflow_core\\examples\\tutorials\\mnist\\input_data.py:328: _DataSet.__init__ (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/_DataSet.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# MNIST 데이터 로드\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1]) # 입력층 [입력데이터개수][28][28][특징의 개수] MNIST 데이터는 회색조 이미지라 채널에 색상이 한개이다.\n",
    "Y = tf.placeholder(tf.float32, [None, 10]) # 출력 (10개의 분류)\n",
    "keep_prob = tf.placeholder(tf.float32) # 드롭아웃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `드롭아웃` : 학습 시 전체 신경망 중 일부만을 사용하도록 하는 것\n",
    "- 학습 단계마다 일부 뉴런을 제거 함으로써, 일부 특징이 특정 뉴런들에 고정되는 것을 막아 가중치의 균형을 잡도록 하여 과적합을 방지\n",
    "- 일부 뉴런을 학습시키지 않기 때문에 신경망이 충분히 학습되기까지의 시간은 조금더 오래 걸리는 편"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 첫 번째 계층 생성\n",
    "### 컨볼루션 계층 만들기\n",
    "- 입력층 X와 첫번째 계층의 가중치 W1을 가지고, 오른쪽과 아래쪽으로 한 칸씩 움직이는 32개의 커널을 가진 컨볼루션 계층을 만듬\n",
    "\n",
    "|첫 계층|이미지 입력(X)|3X3 컨볼루션|2X2 풀링|\n",
    "|-----|---------------|---------------|---------------|\n",
    "|크기|28X28|28X28X32|14X14X32|\n",
    "|설명|이미지크기|컨볼루션 결과X커널개수|풀링결과X커널개수|\n",
    "|이해|MNIST 제공 이미지 크기|3X3`윈도우`가 1칸씩 총 28X28번 이동|2X2에서 하나만 선택 = 14X14의 결과|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([3,3,1,32],stddev=0.01))\n",
    "L1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding='SAME') # padding='SAME' : 이미지의 가장 외곽에서 한칸 밖으로 움직이는 옵션 테두리까지 정확하게 평가가능\n",
    "L1 = tf.nn.relu(L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀링 계층 만들기\n",
    "- 위의 컨볼루션 계층을 입력층으로 사용하고 커널 크기를 2X2로하는 풀링 계층 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME') #  strides=[1,2,2,1]:슬라이딩시 두칸씩 움직이겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 두 번째 계층 생성\n",
    "- 3x3 크기의 커널 64개로 구성한 컨볼루션 계층과 2x2크기의 풀링 계층으로 구성\n",
    "\n",
    "|두 번째 계층|L1 풀링 특징 맵|L2 컨볼루션(3X3)|L2 풀링(2X2)|\n",
    "|-----|---------------|---------------|---------------|\n",
    "|크기|14X14X32|14X14X64|7X7X64|\n",
    "|설명|L1 풀링의 결과|L2 컨볼루션 결과X커널개수|L2 풀링결과X커널개수|\n",
    "|이해|첫번째 계층에서 찾은 특징|3X3`윈도우`가 1칸씩 총 14X14번 이동|2X2에서 하나만 선택 = 14X14의 결과|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([3,3,32,64],stddev=0.01)) # 32는 첫 번째 컨볼루션 계층의 커널 개수, 출력층의 개수, 첫번째 컨볼루션 계층이 찾아낸 이미지의 특징 개수\n",
    "L2 = tf.nn.conv2d(L1,W2,strides=[1,1,1,1],padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 계층 생성\n",
    "- 10개의 분류를 만들어냄\n",
    "- 인접한 계층의 모든 뉴런과 상호 연결된 계층을 `완전 연결 계층`이라고 함\n",
    "\n",
    "|완전 연결 계층|L2 풀링 특징 맵|10개의 분류 계층|\n",
    "|--------|---------------|---------------|\n",
    "|크기|7X7X64|7X7X64(1차원)|\n",
    "|설명|L2 풀링의 결과|L2 컨볼루션 결과X커널개수|\n",
    "|이해|CNN은 2D평면행렬에서 이루어진다. 첫 시작시 입력데이터의 개수X이미지의 특징개수 인 2차원 평면 행렬로 시작 했다.|10개의 분류는 1차원이므로 차원축소|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-3fac5f2e1f84>:5: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "W3 = tf.Variable(tf.random_normal([7*7*64,256],stddev=0.01))\n",
    "L3 = tf.reshape(L2, [-1,7*7*64]) # 직전풀링 계층의 크기가 7x7x64이므로 7x7x64의 1차원 계층으로 만듬\n",
    "L3 = tf.matmul(L3,W3)\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.dropout(L3,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W4 = tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "model = tf.matmul(L3,W4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "# optimizer = tf.train.RMSPropOptimizer(0.001,0.9).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 및 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Avg. cost = 0.340\n",
      "Epoch: 0002 Avg. cost = 0.105\n",
      "Epoch: 0003 Avg. cost = 0.074\n",
      "Epoch: 0004 Avg. cost = 0.059\n",
      "Epoch: 0005 Avg. cost = 0.048\n",
      "Epoch: 0006 Avg. cost = 0.041\n",
      "Epoch: 0007 Avg. cost = 0.036\n",
      "Epoch: 0008 Avg. cost = 0.029\n",
      "Epoch: 0009 Avg. cost = 0.028\n",
      "Epoch: 0010 Avg. cost = 0.024\n",
      "Epoch: 0011 Avg. cost = 0.022\n",
      "Epoch: 0012 Avg. cost = 0.020\n",
      "Epoch: 0013 Avg. cost = 0.018\n",
      "Epoch: 0014 Avg. cost = 0.017\n",
      "Epoch: 0015 Avg. cost = 0.015\n",
      "최적화 완료\n",
      "정확도: 0.9905\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape(-1,28,28,1)\n",
    "        \n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob:0.7})\n",
    "        \n",
    "        total_cost += cost_val\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch +1),'Avg. cost =','{:.3f}'.format(total_cost / total_batch))\n",
    "\n",
    "print('최적화 완료')\n",
    "\n",
    "# 결과 확인\n",
    "is_correct = tf.equal(tf.argmax(model,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도:', sess.run(accuracy, feed_dict={X:mnist.test.images.reshape(-1,28,28,1),Y:mnist.test.labels,keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ch7.3\n",
    "### 고수준 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layers 모듈을 이용하면 컨볼루션 및 풀링 계층의 형성하는 아래의 코드를\n",
    "```python\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32],stddev=0.01))\n",
    "L1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')\n",
    "```\n",
    "아래처럼 간략하게 작성할 수 있다.\n",
    "```python\n",
    "L1 = tf.layers.conv2d(X,32,[3,3],activation=nn.tf.relu,padding='SAME')\n",
    "L1 = tf.layers.max_pooling2d(L1,[2,2],[2,2],padding='SAME')\n",
    "```\n",
    "\n",
    "완전연결계층을 만드는 부분의 코드는\n",
    "```python\n",
    "W3 = tf.Variable(tf.random_normal([7*7*64,256],stddev=0.01))\n",
    "L3 = tf.reshape(L2, [-1,7*7*64]) # 직전풀링 계층의 크기가 7x7x64이므로 7x7x64의 1차원 계층으로 만듬\n",
    "L3 = tf.matmul(L3,W3)\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.dropout(L3,keep_prob)\n",
    "```\n",
    "아래처럼 간략하게 작성할 수 있다.\n",
    "```python\n",
    "L3 = tf.contrib.layers.flatten(L2)\n",
    "L3 = tf.layers.dense(L3,256,activation=tf.nn.relu)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~ layers 모듈을 적용한 코드 ~~\n",
    "# tensorflow.compat.v1 사용시 contrib를 사용할 수 없다....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# MNIST 데이터 로드\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1]) # 입력층 [입력데이터개수][28][28][특징의 개수] MNIST 데이터는 회색조 이미지라 채널에 색상이 한개이다.\n",
    "Y = tf.placeholder(tf.float32, [None, 10]) # 출력 (10개의 분류)\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "L1 = tf.layers.conv2d(X,32,[3,3],activation=tf.nn.relu,padding='SAME')\n",
    "L1 = tf.layers.max_pooling2d(L1,[2,2],[2,2],padding='SAME')\n",
    "L1 = tf.layers.dropout(L1,0.7,is_training)\n",
    "\n",
    "L2 = tf.layers.conv2d(L1,64,[3,3])\n",
    "L2 = tf.layers.max_pooling2d(L2,[2,2],[2,2])\n",
    "L2 = tf.layers.dropout(L2,0.7,is_training)\n",
    "\n",
    "\n",
    "L3 = tf.contrib.layers.flatten(L2)\n",
    "L3 = tf.layers.dense(L3,256,activation=tf.nn.relu)\n",
    "L3 = tf.layers.dropout(L3,0.5, is_training)\n",
    "\n",
    "model = tf.layers.dense(L3,10,activation=None)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "# optimizer = tf.train.RMSPropOptimizer(0.001,0.9).minimize(cost)\n",
    "\n",
    "# 학습 시작\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape(-1,28,28,1)\n",
    "        \n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob:0.7})\n",
    "        \n",
    "        total_cost += cost_val\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch +1),'Avg. cost =','{:.3f}'.format(total_cost / total_batch))\n",
    "\n",
    "print('최적화 완료')\n",
    "\n",
    "# 결과 확인\n",
    "is_correct = tf.equal(tf.argmax(model,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도:', sess.run(accuracy, feed_dict={X:mnist.test.images.reshape(-1,28,28,1),Y:mnist.test.labels,keep_prob: 1}))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화 함수 변경 \n",
    "#### AdamOptimizer에서 RMSPropOptimizer로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python3.7\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch: 0001 Avg. cost = 0.946\n",
      "Epoch: 0002 Avg. cost = 0.097\n",
      "Epoch: 0003 Avg. cost = 0.063\n",
      "Epoch: 0004 Avg. cost = 0.048\n",
      "Epoch: 0005 Avg. cost = 0.038\n",
      "Epoch: 0006 Avg. cost = 0.033\n",
      "Epoch: 0007 Avg. cost = 0.029\n",
      "Epoch: 0008 Avg. cost = 0.024\n",
      "Epoch: 0009 Avg. cost = 0.021\n",
      "Epoch: 0010 Avg. cost = 0.019\n",
      "Epoch: 0011 Avg. cost = 0.017\n",
      "Epoch: 0012 Avg. cost = 0.016\n",
      "Epoch: 0013 Avg. cost = 0.013\n",
      "Epoch: 0014 Avg. cost = 0.013\n",
      "Epoch: 0015 Avg. cost = 0.012\n",
      "최적화 완료\n",
      "정확도: 0.9907\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# MNIST 데이터 로드\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1]) # 입력층 [입력데이터개수][28][28][특징의 개수] MNIST 데이터는 회색조 이미지라 채널에 색상이 한개이다.\n",
    "Y = tf.placeholder(tf.float32, [None, 10]) # 출력 (10개의 분류)\n",
    "keep_prob = tf.placeholder(tf.float32) # 드롭아웃\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32],stddev=0.01))\n",
    "L1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding='SAME') # padding='SAME' : 이미지의 가장 외곽에서 한칸 밖으로 움직이는 옵션 테두리까지 정확하게 평가가능\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME') #  strides=[1,2,2,1]:슬라이딩시 두칸씩 움직이겠다.\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([3,3,32,64],stddev=0.01)) # 32는 첫 번째 컨볼루션 계층의 커널 개수, 출력층의 개수, 첫번째 컨볼루션 계층이 찾아낸 이미지의 특징 개수\n",
    "L2 = tf.nn.conv2d(L1,W2,strides=[1,1,1,1],padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([7*7*64,256],stddev=0.01))\n",
    "L3 = tf.reshape(L2, [-1,7*7*64]) # 직전풀링 계층의 크기가 7x7x64이므로 7x7x64의 1차원 계층으로 만듬\n",
    "L3 = tf.matmul(L3,W3)\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.dropout(L3,keep_prob)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "model = tf.matmul(L3,W4)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "#optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.001,0.9).minimize(cost)\n",
    "\n",
    "# 학습 시작\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape(-1,28,28,1)\n",
    "        \n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob:0.7})\n",
    "        \n",
    "        total_cost += cost_val\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch +1),'Avg. cost =','{:.3f}'.format(total_cost / total_batch))\n",
    "\n",
    "print('최적화 완료')\n",
    "\n",
    "# 결과 확인\n",
    "is_correct = tf.equal(tf.argmax(model,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도:', sess.run(accuracy, feed_dict={X:mnist.test.images.reshape(-1,28,28,1),Y:mnist.test.labels,keep_prob: 1}))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAAAqCAYAAAB7uVNDAAADwUlEQVR4Ae1W247sMAib///pOeJIVi3WXJK226yakSKIsYFQHubz3b9XT+Dz6tfvx3/3Arx8CfYC7AWoJ/D5fL7+sMpi0c/r+A5Npgcns1fouS+fz9+zXv5aLP5yAy/pDijiRThasLg/iJmt9Mz1fqRlnH2lt3h2vGale7kAnYdlA8JjkQd3th09871/Rh9pGWff1+7cz+o7NWY55QJw4ughEa604Jrlw1z4HI9841ps9hdpGWd/ps5Z/UzNrmZoctFDIhxNcJx9FQc2YlXOUb3l4MP6Kj/rlM+5VvPbC4AhwPJDFIa4innM36GFtbg/iJmt9MzN/CgP1/b6SON5q95bC+AfWd3xWM8DbpZj7DPH8zjGGvNxmDPqc07WRrhxshjnWNVPFyAbKsfODiHTRzHG2R8ZtOmq0/nIVY7Z/kbeMstNF8AnjR4S4V4/e1cD5ly/UZ/rKV/1oDClfRK7fQHUx/NYdwDRQCO8k7ejneV0dJ0e7+TcvgCd5ruDingRfmXtKpfqQWFVnt+OpwtgD6iONXz2oZG+qg0d7Mzwzmo7PYIz09/dmnQBusXPDNFqrKDHR4psdxZ/jXfJAvy1R+9+jwnsBThm8UpvL8ArP/vx6L0Axyxe6e0FeOVnPx5dLkD0r5j/ubN/pP7pRbwI/5lhI1dPoFwALhh9qAy3WHYsf6RH7UyPGLhXW+SHHc0PHazXA4cdiUPjrc+R3dsLgCIqmcXO/DJ9FjtTs6NVtRUW5VJcxthHDsbY78bB69r2l0MzbM3HyQqCw5b5hke/LBZpPD6bQ+kU5uvhrriMsa80M3Hk6dp48pSBG2EfFIVVMdaYjwMdLPOAjdrZHEqnsKgfxWWMfeRgjP1uHLyuLRegasIKKQ43YHF/fJzv7Fe5mXu1r2orLKqruIyxjxyMsR/FjcMHvK5NF0A1gMQcYx9xZSNehCOHxasD7pVW9aWwqKbiMsY+cjDG/kwcmsymC5AJOaYatbjh1QGP863iq3cpLOtXvZ/5WVzVUpjPx/fKv2QBqiJXx6sh+HqjfOiVTmHgd2yl5zj7yK0wxMxWceb+53tA3S1pdpQGWKZDDNyuNd3Ib5SP3EqnMPA7ttJznH3kZox9FQeW2XKSqohP2OF4Dd9H9aN8rjXiqzpdzOpU3LvjnbcuvQA2oJHTefAox9dXevUhwav0d8fRR2TLBTChb9Lfo+Rd3PLt3zMT2JN/Zu7LVN0LsMyneKaRvQDPzH2ZqnsBlvkUzzSyF+CZuS9TdS/AMp/imUb2Ajwz92Wq/gMtW6GZLmccwwAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적화 함수에 따른 정확도 차이\n",
    "- dropout 0.7로 L3만 적용\n",
    "- 손실함수 softmax_cross_entropy_with_logits_v2 사용\n",
    "\n",
    "#### AdamOptimizer 의 정확도\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHwAAAApCAYAAAD3XU36AAADwElEQVR4Ae1W0Y4jMQjr//90TzxYY7E2IZO2185kpAhibCCwK/Xx3N+tJvC41Wv3Y5974Tf7Ixgu/PF4PPPhGUXMfVnHd2gqPTiVfYWe+8r58r3q5RdiflvN7rsDcTyHo3zE80Es7EjP3Ow7LePsK33Eq5M1//teLrzzkGogeBzy4M62o2d+9lf0Tss4+7l2576q79SY4ZQL50SucYcrLbhh+TAXPsedH9yInf2clnH2z9RZ1Z+pWWna03KNOxxFOc6+igObsSrnrD5y8GH9KD/rlM+5vsFvLRyPhuXGFYa4imUs36GFjXg+iIUd6Zlb+S4P1856p8m8b7oPF54fNbrjcZkHPCzH2GdO5nGMNeHjMGfW55ysdXhwqhjn+CbfLrwaIsdWH13pXYxx9mcGG7rR6Sx1lONsfzNvmeHaheckrnGHZ/3Zuxoo5/pEfa6nfNWDwpT209hbF66WlbHug90AHd7J29Ge5XR0nR5fzXnrwjvNdgfjeA5/Ze1RLtWDwkZ5PhG3C4+GRycaXH2Y049qQwd7Zlir2k6P4Jzp7x0au/BusZWhRY1v0GMpznZn8Qu85YX/wiN3j8cE9sKPWdzC2wu/xZqPR+6FH7O4hbcXfos1H48sF+5+tfIva/aPtH89x3P43wwbecUEyoVzAbeYCo9YdSK/06N2pUcM3Fdb5IedzQ8dbNYDh52JQ5NtzpHvrYUjaRbHPWIrX6WvYis1O1pVW2Eul+Iyxj5yMMa+igNjqzQcD7+1LSRiGz5OTsp3cNjmON/ZD83qdzaH0inM9ae4jLGPHIyxr+LAYBUfMbbDiXIi9pFEYaMYa8LHgQ6WecBm7dkcSqcw14/iMsY+cjDGvooDg1V8xNiWC1dJMpbvnDz8iOfDnEpfxTjHO3xVW2GutuIyxj5yMMa+igMLq7gcZ98uvErCMfY5cfYdz+HQR3x0wH2lVX0pzNVUXMbYRw7G2FdxYGEVl+Ps24UzqfJdscBHJ/I6fVXzEzHVl8KqXtT7mV/FVS2FRT6Hcy34ywtHok/ZmcdFT7N8vEPpFAZ+x470HGcfubsY+MoOFx5FqqOSAqt0iIHbtaGb+Wb5yK10CgO/Y0d6jrOP3F0MfGXL6akCOUmHkzV8n9XP8rnWjK/qdLGoM+KuxvEWlQcxZb924fGQmaMet4rl+ipfNfCRfjUe/VT1Zb8KZCw3le/MPePPNnymxtYcEyj/ww/a9q4ygb3wq2yy+Y698OagrkLbC7/KJpvv2AtvDuoqtL3wq2yy+Y698OagrkL7B7lZM898FY+iAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSPropOptimizer 의 정확도\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 계층에서 Dropout 수행\n",
    "- 최적화 함수 RMSPropOptimizer 사용\n",
    "- 손실함수 softmax_cross_entropy_with_logits_v2 사용\n",
    "- L1,L2,L3 모든 층에서 70% Dropout 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 Avg. cost = 0.930\n",
      "Epoch: 0002 Avg. cost = 0.122\n",
      "Epoch: 0003 Avg. cost = 0.083\n",
      "Epoch: 0004 Avg. cost = 0.067\n",
      "Epoch: 0005 Avg. cost = 0.056\n",
      "Epoch: 0006 Avg. cost = 0.051\n",
      "Epoch: 0007 Avg. cost = 0.046\n",
      "Epoch: 0008 Avg. cost = 0.043\n",
      "Epoch: 0009 Avg. cost = 0.040\n",
      "Epoch: 0010 Avg. cost = 0.039\n",
      "Epoch: 0011 Avg. cost = 0.035\n",
      "Epoch: 0012 Avg. cost = 0.036\n",
      "Epoch: 0013 Avg. cost = 0.031\n",
      "Epoch: 0014 Avg. cost = 0.032\n",
      "Epoch: 0015 Avg. cost = 0.034\n",
      "최적화 완료\n",
      "정확도: 0.9918\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# MNIST 데이터 로드\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1]) # 입력층 [입력데이터개수][28][28][특징의 개수] MNIST 데이터는 회색조 이미지라 채널에 색상이 한개이다.\n",
    "Y = tf.placeholder(tf.float32, [None, 10]) # 출력 (10개의 분류)\n",
    "keep_prob = tf.placeholder(tf.float32) # 드롭아웃\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32],stddev=0.01))\n",
    "L1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding='SAME') # padding='SAME' : 이미지의 가장 외곽에서 한칸 밖으로 움직이는 옵션 테두리까지 정확하게 평가가능\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME') #  strides=[1,2,2,1]:슬라이딩시 두칸씩 움직이겠다.\n",
    "L1 = tf.nn.dropout(L1,keep_prob)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([3,3,32,64],stddev=0.01)) # 32는 첫 번째 컨볼루션 계층의 커널 개수, 출력층의 개수, 첫번째 컨볼루션 계층이 찾아낸 이미지의 특징 개수\n",
    "L2 = tf.nn.conv2d(L1,W2,strides=[1,1,1,1],padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')\n",
    "L2 = tf.nn.dropout(L2,keep_prob)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([7*7*64,256],stddev=0.01))\n",
    "L3 = tf.reshape(L2, [-1,7*7*64]) # 직전풀링 계층의 크기가 7x7x64이므로 7x7x64의 1차원 계층으로 만듬\n",
    "L3 = tf.matmul(L3,W3)\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.dropout(L3,keep_prob)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256,10],stddev=0.01))\n",
    "model = tf.matmul(L3,W4)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "#optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.001,0.9).minimize(cost)\n",
    "\n",
    "# 학습 시작\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape(-1,28,28,1)\n",
    "        \n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob:0.7})\n",
    "        \n",
    "        total_cost += cost_val\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch +1),'Avg. cost =','{:.3f}'.format(total_cost / total_batch))\n",
    "\n",
    "print('최적화 완료')\n",
    "\n",
    "# 결과 확인\n",
    "is_correct = tf.equal(tf.argmax(model,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도:', sess.run(accuracy, feed_dict={X:mnist.test.images.reshape(-1,28,28,1),Y:mnist.test.labels,keep_prob: 1}))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIMAAAAoCAYAAADdRklLAAADyklEQVR4Ae1X244rMQjr///0HiEdayzW3CbTbjtNpQhibCAMD7uPn/3bE/g/gceexJ4AJtBahsfj8eMPEpi1WPTzOr5Dk+nByewVeu7L5/P3rJdPjsVfcfCq7rAiXoSjBYv7g5jZSs9c70daxtlXeotnx2ve9V4uQ+eR2bDwcOTBnW1Hz3zvr+gjLePs+9qd+6q+U+MKTrkMXCR6VIQrLbhm+TAXPscj37gWO/uLtIyzf6bOqv5MzTOa0RSjR0U4GuI4+yoObGJVzqnecvBhfZWfdcrnXO/st5cBA4HlRykMcRXzmL9DC2txfxAzW+mZm/lRHq7t9ZHG8z7h3loG/+Dqjod7HnCzHGOfOZ7HMdaYj8Ocqc85WRvhxslinOMT/HQZsgFzbHUgmT6KMc7+ZOimq07ng1c5zvY3ecsV3HQZfIHoURHu9Wfvatic6xX1uZ7yVQ8KU9p3wZ6+DOpDeqw7jGi4Ed7J29Ge5XR0nR5fxXn6MnQe0h1axIvwK2tXuVQPCqvy/GU8XQZ7THWs+dVHR/qqNnSwZwa5qu30CM6Z/l6pSZeh28jKQK3GO+jxwSLbncUn8y5Zhk8ewO79mMBehmMWX+/tZfj6FTgGsJfhmMXXe3sZvn4FjgGUyxD9dc3/AbB/pP7tRbwI/51hI8+cQLkMXDz6aBlusexY/kiP2pkeMXCvtsgPO80PHazXA4edxKHx1ufo3tvLgIIqscVWfpk+i63U7GhVbYVFuRSXMfaRgzH2VRwYW6XheOa3vyKKsDUfJy1CPMVHTpUjiym+ws7mUDqFqZqGKS5j7CMHY+yrODBYxUesY1vLwEXYRwGFVTHWmI8DHSzzgE3t2RxKp7CoH8VljH3kYIx9FQcGq/iIdWy5DKqAx/zdF7a4P8zJ9FmMczzDV7UVFtVWXMbYRw7G2FdxYGYVl+MdP12GrADH2M+KRrwIRy6LVwfcK63qS2FRTcVljH3kYIx9FQdmVnE53vHTZegkyBqxBquT6bv1n8VTA1ZYVl+9n/lZXNVSmOWLcK5V+ZcsQ1Xk6vj04VM++lU6hYHfsZWe4+wjdxcDf2Jby2ANZCcrmOkQy/QqZrrJb8pHbqVTGPgdW+k5zj5ydzHwJ7acqiruC3Q4XsP3qX7K51oTX9XpYlan4q7G8RaVB7GJfetlsEdOzuThXa6vr3TZx6j0q3HrJ6uv+o2wchlQzDfN9yh5F7/qMd16m6cn0FoGLd3o3Sawl+FuX3ThPXsZFoZ3N+lehrt90YX37GVYGN7dpHsZ7vZFF96zl2FheHeT/gNrVKF9uYgIAAAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout 적용 계층 수에따른 정확도 차이\n",
    "- 최적화 함수 RMSPropOptimizer 사용\n",
    "- 손실함수 softmax_cross_entropy_with_logits_v2 사용\n",
    "- Dropout 비율은 0.7\n",
    "\n",
    "#### L3계층에서만 사용\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHkAAAAnCAYAAAArfufOAAADr0lEQVR4Ae1Wi27jMAzr///0DipAlFVIWU5yXdI5gGGZIinZwh6Pn/V9/Qs8vv6G64I/csiPx+MnL36ryLkv6/gMTaUHp9rP0HNf2S+fq17ukPPTKrrvPoLjORwlI58XcrGP9MzNsdMyzrHSR75aWfPb582QO81Xj4ALwQdn3jt65uf4iN5pGec41+6cj+o7NWY4myGz2DXrcKUFN3ZezEXMeRcHN3J7P6dlnOM9dY7q99SsNOVruWYdjkKc51jlgc3synNWHx68WD/yZ52K2esKsR0yLoqdm1UY8iqXsXyGFnvk80Iu9pGeuVXsfLh21jtN5l3pLIecLzI640KZBzx2znHMnMzjHGsixmLObMyerHV4cKoce1wpfhty9XCcO3rRSu9yjHM885ihG63OIEcee/ubucsM923IWeiadXjW7z2rR2SvT9TneipWPShMaT+NnT5kNaCMdS/pHs3hHd+Odi+no+v0eDbn9CF3Guw+huM5/MzaIy/Vg8JGPp/Ivw05mhytaOroZZx+VBs67Hse6Ki20yM4e/r7H5q3IXcLHHmoqHEFPQbh9u5b3IG3a8h3uNjq8fUCa8ivt/jaaA35a0f7utga8ustvjZaQ/7a0b4uthmy+2+T/yPm+GW1jRzP4VuHhZzxApshs6kbRoVHrlrh7/SoXemRA/fsHf7YZ/2hw571wLHnfJwj5z7osDse49atMqmaYHMXV/oq5/zOwlVthbl6issYx/DIWJwz5riBOy40Tw4fOIaY94ixmJtjcHhnTuDuq3JOk/G9HkqnsFwPZ8VljOOuBrzYu3rWPHUZyGazxoqvPIOnuApTPVbYXg+lU5irrbiMcQyPLhb8GS78nzo+dI1UMfaJfF45z2eOR97MPTtWtRXm6iouYxzDo4sFf4YL/6fu7dD8NaqKsQ9ix3M464JTLXDP3FVfCnM1FZcxjuHRxZgfGizg1e7/OBYq1VjQUbjawSvsfy2l7qWwqkF1d+aP8sF1NRWuMK739MvAFc+di3Dfs3xolU5h4Hf2kV7lFRa1FK6w3Jf8SQ5htbIJnysdcszvxKGb+Wb58FY6hYHf2Ud6lVdY1FK4wnJfm9driSYffVN0Ut/pKdfYc1Z1uljUG3FHefSseB1/6PN+qSHH5WZWvswZ51xfebohBHekH+Xhoeoixx6Ox/hmyMqITSM++p3hcbSHv6Q/PrG/9Fo3vesa8k0HN9P2GvLMa92Uu4Z808HNtL2GPPNaN+WuId90cDNt/wO6e4sjYh4UVQAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모든계층에서 사용\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 커널 개수 변경\n",
    "- 최적화 함수 RMSPropOptimizer 사용\n",
    "- 손실함수 softmax_cross_entropy_with_logits_v2 사용\n",
    "- L1,L2,L3 모든 층에서 70% Dropout 사용\n",
    "- 커널개수 첫번째계층 `32->64`개로 두번째계층 `64->128`개로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 Avg. cost = 0.842\n",
      "Epoch: 0002 Avg. cost = 0.075\n",
      "Epoch: 0003 Avg. cost = 0.051\n",
      "Epoch: 0004 Avg. cost = 0.039\n",
      "Epoch: 0005 Avg. cost = 0.035\n",
      "Epoch: 0006 Avg. cost = 0.030\n",
      "Epoch: 0007 Avg. cost = 0.027\n",
      "Epoch: 0008 Avg. cost = 0.024\n",
      "Epoch: 0009 Avg. cost = 0.023\n",
      "Epoch: 0010 Avg. cost = 0.022\n",
      "Epoch: 0011 Avg. cost = 0.022\n",
      "Epoch: 0012 Avg. cost = 0.019\n",
      "Epoch: 0013 Avg. cost = 0.019\n",
      "Epoch: 0014 Avg. cost = 0.018\n",
      "Epoch: 0015 Avg. cost = 0.016\n",
      "최적화 완료\n",
      "정확도: 0.9921\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# MNIST 데이터 로드\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1]) # 입력층 [입력데이터개수][28][28][특징의 개수] MNIST 데이터는 회색조 이미지라 채널에 색상이 한개이다.\n",
    "Y = tf.placeholder(tf.float32, [None, 10]) # 출력 (10개의 분류)\n",
    "keep_prob = tf.placeholder(tf.float32) # 드롭아웃\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,64],stddev=0.01))\n",
    "L1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding='SAME') # padding='SAME' : 이미지의 가장 외곽에서 한칸 밖으로 움직이는 옵션 테두리까지 정확하게 평가가능\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME') #  strides=[1,2,2,1]:슬라이딩시 두칸씩 움직이겠다.\n",
    "L1 = tf.nn.dropout(L1,keep_prob)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([3,3,64,128],stddev=0.01)) # 32는 첫 번째 컨볼루션 계층의 커널 개수, 출력층의 개수, 첫번째 컨볼루션 계층이 찾아낸 이미지의 특징 개수\n",
    "L2 = tf.nn.conv2d(L1,W2,strides=[1,1,1,1],padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')\n",
    "L2 = tf.nn.dropout(L2,keep_prob)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([7*7*128,512],stddev=0.01))\n",
    "L3 = tf.reshape(L2, [-1,7*7*128]) \n",
    "L3 = tf.matmul(L3,W3)\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.dropout(L3,keep_prob)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([512,10],stddev=0.01))\n",
    "model = tf.matmul(L3,W4)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=Y))\n",
    "#optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.001,0.9).minimize(cost)\n",
    "\n",
    "# 학습 시작\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape(-1,28,28,1)\n",
    "        \n",
    "        _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob:0.7})\n",
    "        \n",
    "        total_cost += cost_val\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch +1),'Avg. cost =','{:.3f}'.format(total_cost / total_batch))\n",
    "\n",
    "print('최적화 완료')\n",
    "\n",
    "# 결과 확인\n",
    "is_correct = tf.equal(tf.argmax(model,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도:', sess.run(accuracy, feed_dict={X:mnist.test.images.reshape(-1,28,28,1),Y:mnist.test.labels,keep_prob: 1}))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHkAAAAjCAYAAACw76XYAAADmUlEQVR4Ae1WAW7jMAzr/z+9QcG4EBopy07apUUKGFIokpIt3HCPr+L3eDy+8mF61Nwv6/gbmkoPThXP0PNc2S9/V7Ncuea31Ji6+wiO53C0jno+qEUc6Zmbc6dlnHOlj3p1sua/vu2SO8NXj4ALwQffHDt65uf8iN5pGec89+58H9V3enQ4dsksdsM6XGnBjciHuci57vLgRm3157SMc77S56h+pafStF7JDetwNOI656oObCYqz1l9ePBh/cifdSpnr//Mh0vGRRF5WIWhrmoZy9/QIkY9H9QijvTMrXLnw72z3mky7wrf5ZLzRUbfuFDmAY/INc6Zk3lcY03kOMyZzdmTtQ4PTlVjjyvkcsnVw3Ht6EUrvasxzvnMY4ZudDqLHHmszjdzlw5XLjkL3bAOz/rVb/WI7PWK/txP5WoGhSntq7CnLVktKGPdS7pHc3jHt6Nd5XR0nRnP4jxtyZ0Bu4/heA4/s/fIS82gsJHPM+tyyTHk6MRQRy/j9KPe0CGuPNBRbWdGcFbmO1Mjl9xtcOShoscV9FiEi923uDLv0JKvfLF7tv0F7iXvb/Gx2b3kj13tfrF7yftbfGx2L/ljV7tfzC7Z/W+T/0fM+W75N3M8h/91uJEjL2CXzKZuGRUeteqEv9Ojd6VHDdyzI/wRZ/2hQ8x64Ii5Ht9Rcz/oEB1v86mKG+FnWYpXDaH4Gav0VS37nP2teivM9VVcxjiHR8biO2OOG7jjbjUIXYSYY+Q4TreZE0/x4ak8qpriK2zVQ+kUpnoGpriMcQ6PLtbxhyei/3uQhp0Zwg2S8fDEwUCIqh9q3bjqoXQKc3MoLmOcw6OLBX+Gu/HRJMeOkeKwT9TzyXX+5nzkzdyzc9VbYa6v4jLGOTy6WPBnuBsfTTgqE9S5xjnqKjqew+ER9dEB98yo5lKY66m4jHEOjy7G/NDgAFex/HOtBIypwaKOxlUEj/2ukqt7KayaV92d+aN6cF1PhSsM/Q4tGSavitVF1AyzfHgoncLA78SRXtUVFr0UrjDMVS45hNWBiYqVDjWlq7DQzfxm+fBWOoWB34kjvaorLHopXGGYy75aJfoVTz46dIidHuBGnOWzdiZXfbqYm5P1nGOuLtbxhyfiJZccF545uMyZMfdX3mox4I30o3r4HPHHHJsPf+Q8D5K/M3/2u7rErNfN9y9g/yV7yV15txe4l/xuG1uY917ywqO9m+Re8rttbGHebyARAkqHxnWgAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 커널 수에따른 정확도 차이\n",
    "- 최적화 함수 RMSPropOptimizer 사용\n",
    "- 손실함수 softmax_cross_entropy_with_logits_v2 사용\n",
    "- Dropout 비율은 0.7, 모든계층에 적용\n",
    "\n",
    "#### 첫 번째 계층 32, 두 번째 계층 64개의 커널 정확도\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHoAAAAoCAYAAAAxH+4YAAADo0lEQVR4Ae1WCW4jMQzL/z+dQgXYYRXqsD25PYBhmSIp2dosernu7yte4PIVt9yXvO5Bf8k/gj3oPegveYEvuWb7F325XK5+8RtZLvq8js/QZHpwsv0MPffl/fw56+UVc/F0BrvtPkTEi3C0YXm/kLO90jPXx5GWcY6V3vLZ8ppHn1uD7lwgewhcCj44897RM9/HK/pIyzjHvnbnvKrv1Mg4rUGzQdRwhCstuLbzYi5izkexcS03+0VaxjmeqbOqn6nJmuHXiRqOcBTjPMcqD2xkV56jevPgxfrKn3UqZq9nxEODxmWxc8MKQ17lPObP0GK3vF/I2V7pmZvFkQ/X9vpI43nPPLcH7S9TnXEpzwNuO+c4Zo7ncY41FmMxZzRmT9ZGuHGyHHs8My4HnT0e51Yvm+mjHOMcjzyo6arVGWblMdvfyF0ybjloL44ajnCvnz2rh2SvR9TneipWPShMae+NPWTQakge6140ergI7/h2tLOcjq7T4yrnIYPuNNl9kIgX4WfWrrxUDwqrfO6RLwdtjVbLGlu9UKSvakOHfeaRVrWdHsGZ6e8MTTnobpGVx7Iar6DHMKK9+xavyDtt0K94ud3T8QJ70MdbfHS0B/3R4z0utwd9vMVHR3vQHz3e43KtQUd/hfJfyhwf9rdRxIvwW4eNzLxAa9BsHA0kwy2XLfOP9Kid6ZED9+wd/thH/aHD7vXAsY/mjW/a7MuzThk10inkrG6OWaNZ7sboZEDVVlhUVnEZ4xgejHGs8oYZR/HA/+XwoYphxjuKAIs8mIeYuZk+y7FHFs96KJ3CotqKyxjH8GCMY5XPMORsb/+iuSDHMFNYlWONxVjQYWcesNF91kPpFBb1o7iMcQwPhSFnu8or7J+GD1GsTDzmz97L8n4xJ9NnOfa4R6xqKyyqrbiMcQwPhVW5TGPa8hedGXCOYzSl9ogX4fCwfLXAPXNXfSksqqm4jHEMD4VZLsKr3G8e5qt71ITh1eo0utrfrF7dS2GZv7o/86u8cauaZZ4LvlNcXczfZZQPvdIpDPzOXul93p9VjYpT/tcNUzPKFnhqz3TIKV2GmW7kG+XDW+kUBn5nr/Sc5zjzrnit16pMrIEOZ6VRr12t5/2is6rTxcyz4s7kVa/Kh3kvP2i7wMjiy50V+/rKN3voSp/lfQ5n34Ph2ZdnSYkC0U7UqbBqdMp0i/5eoD3oP8UO3vIF9qDfcmzjTe9Bj7/ZWyr2oN9ybONN/wATJCG3OeDGMQAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫 번째 계층 64, 두 번째 계층 128개의 커널 정확도\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "- 학습시간이 체감상 4배이상 걸린 것 같지만 후자의 경우 3번째 Epoch까지 손실값이 큰폭으로 감소했고, 정확도도 조금 더 향상됬다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정확도가 가장 높았던 결과\n",
    "- 최적화 함수 RMSPropOptimizer 사용\n",
    "- 손실함수 softmax_cross_entropy_with_logits_v2 사용\n",
    "- Dropout 비율은 0.7, 모든계층에 적용\n",
    "- 첫 번째 계층 64, 두 번째 계층 128개의 커널 정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 과적합인지 판별할 수 있는 방법을 알아봐야겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
