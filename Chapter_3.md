# Chapter 3

## ch3.1
### 텐서와 그래프 실행


```python
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
```

    WARNING:tensorflow:From c:\python3.7\lib\site-packages\tensorflow_core\python\compat\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
    Instructions for updating:
    non-resource variables are not supported in the long term
    


```python
hello = tf.constant('Hello,TensorFlow!')
print(hello)
```

    Tensor("Const:0", shape=(), dtype=string)
    


```python
sess = tf.Session()
a = tf.constant(10)
b = tf.constant(32)
c = tf.add(a,b)
print(c)
```

    Tensor("Add:0", shape=(), dtype=int32)
    


```python
print(sess.run(hello))
print(sess.run([a,b,c]))

sess.close()
```

    b'Hello,TensorFlow!'
    [10, 32, 42]
    

## ch3.2
### placeholder and variable


```python
X = tf.placeholder(tf.float32, [None,3])
print(X)
```

    Tensor("Placeholder:0", shape=(?, 3), dtype=float32)
    


```python
x_data = [[1,2,3],[4,5,6]]
```


```python
W = tf.Variable(tf.random_normal([3,2]))
B = tf.Variable(tf.random_normal([2,1]))
#W = tf.Variable([[0.1,0.1],[0.2,0.2],[0.3,0.3]])
```


```python
expr = tf.matmul(X,W)+b
print(expr)
```

    Tensor("add_3:0", shape=(2, 2), dtype=float32)
    


```python
sess = tf.Session()
sess.run(tf.global_variables_initializer())

print("===x-data===")
print(x_data)
print("===W===")
print(sess.run(W))
print("===B===")
print(sess.run(b))
print("===expr===")
print(sess.run(expr,feed_dict={X:x_data}))

sess.close()
```

    ===x-data===
    [[1, 2, 3], [4, 5, 6]]
    ===W===
    [[ 0.53080404  0.13427517]
     [ 0.5496091   0.4894694 ]
     [-0.04445903 -0.94626844]]
    ===B===
    [[-0.506226  ]
     [-0.67812455]]
    ===expr===
    [[ 0.9904192 -2.2318172]
     [ 3.926383  -3.3712873]]
    

## ch3.3
### 선형 회귀 모델


```python
x_data = [1,2,3]
y_data = [1,2,3]
```


```python
W = tf.Variable(tf.random_uniform([1],-0.1,1.0))
b = tf.Variable(tf.random_uniform([1],-0.1,1.0))
```


```python
X = tf.placeholder(tf.float32, name = "X")
Y = tf.placeholder(tf.float32, name = "Y")
```


```python
hypothesis = W * X + b
```


```python
cost = tf.reduce_mean(tf.square(hypothesis -Y))
```


```python
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train_op = optimizer.minimize(cost)
```


```python
sess = tf.Session()
sess.run(tf.global_variables_initializer())
for step in range(100):
    _,cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})
    print(step, cost_val,sess.run(W), sess.run(b))
```

    0 3.0706031 [0.92620194] [0.3691554]
    1 0.0527193 [0.84741795] [0.32484353]
    2 0.015908135 [0.85989046] [0.32090762]
    3 0.014742677 [0.86229634] [0.31276992]
    4 0.014037493 [0.8657118] [0.3052974]
    5 0.013370636 [0.8689285] [0.2979532]
    6 0.012735531 [0.8720806] [0.29079115]
    7 0.012130591 [0.87515557] [0.28380066]
    8 0.011554371 [0.8781568] [0.2769783]
    9 0.011005531 [0.8810858] [0.27031994]
    10 0.010482769 [0.8839444] [0.26382163]
    11 0.009984825 [0.88673437] [0.25747955]
    12 0.009510529 [0.88945717] [0.2512899]
    13 0.009058782 [0.8921145] [0.24524906]
    14 0.008628481 [0.89470804] [0.23935346]
    15 0.008218621 [0.8972392] [0.23359957]
    16 0.007828222 [0.8997094] [0.22798397]
    17 0.0074563907 [0.90212035] [0.22250341]
    18 0.007102199 [0.9044733] [0.21715458]
    19 0.006764844 [0.90676975] [0.21193434]
    20 0.0064435024 [0.90901095] [0.20683958]
    21 0.0061374344 [0.9111982] [0.20186727]
    22 0.005845893 [0.91333294] [0.19701453]
    23 0.005568217 [0.91541636] [0.19227843]
    24 0.0053037126 [0.9174497] [0.1876562]
    25 0.005051788 [0.9194342] [0.18314508]
    26 0.0048118196 [0.9213709] [0.17874238]
    27 0.0045832577 [0.9232611] [0.17444554]
    28 0.0043655527 [0.92510587] [0.17025198]
    29 0.004158181 [0.9269062] [0.16615923]
    30 0.0039606676 [0.9286634] [0.1621649]
    31 0.003772531 [0.93037826] [0.15826656]
    32 0.0035933377 [0.9320519] [0.15446194]
    33 0.0034226535 [0.93368536] [0.15074879]
    34 0.0032600716 [0.9352795] [0.14712489]
    35 0.003105217 [0.93683535] [0.14358811]
    36 0.0029577166 [0.9383538] [0.14013635]
    37 0.002817224 [0.9398357] [0.13676758]
    38 0.0026834004 [0.94128203] [0.13347977]
    39 0.0025559396 [0.94269353] [0.13027099]
    40 0.0024345303 [0.9440712] [0.12713937]
    41 0.0023188887 [0.9454157] [0.12408305]
    42 0.0022087435 [0.9467278] [0.12110016]
    43 0.0021038167 [0.9480085] [0.11818901]
    44 0.0020038886 [0.94925827] [0.11534781]
    45 0.0019087015 [0.9504781] [0.11257495]
    46 0.0018180358 [0.9516685] [0.10986871]
    47 0.0017316815 [0.9528304] [0.10722756]
    48 0.0016494197 [0.95396435] [0.1046499]
    49 0.001571069 [0.955071] [0.10213417]
    50 0.0014964495 [0.95615107] [0.09967893]
    51 0.0014253622 [0.9572052] [0.09728272]
    52 0.0013576644 [0.95823395] [0.09494411]
    53 0.0012931685 [0.959238] [0.09266172]
    54 0.0012317445 [0.9602179] [0.09043419]
    55 0.0011732352 [0.96117425] [0.0882602]
    56 0.0011175033 [0.96210754] [0.08613847]
    57 0.0010644243 [0.9630185] [0.08406777]
    58 0.0010138581 [0.9639074] [0.08204682]
    59 0.00096570404 [0.96477515] [0.0800745]
    60 0.0009198275 [0.9656219] [0.07814953]
    61 0.0008761363 [0.9664483] [0.07627089]
    62 0.0008345226 [0.9672549] [0.07443739]
    63 0.0007948799 [0.968042] [0.07264797]
    64 0.0007571193 [0.96881026] [0.07090155]
    65 0.0007211589 [0.9695601] [0.06919713]
    66 0.00068690255 [0.97029185] [0.06753368]
    67 0.00065427547 [0.971006] [0.06591019]
    68 0.00062319584 [0.971703] [0.06432576]
    69 0.00059359317 [0.97238326] [0.06277942]
    70 0.0005653975 [0.97304714] [0.06127023]
    71 0.0005385421 [0.9736951] [0.05979734]
    72 0.0005129596 [0.9743274] [0.05835982]
    73 0.0004885912 [0.97494453] [0.05695691]
    74 0.00046538527 [0.97554684] [0.05558771]
    75 0.00044327983 [0.97613466] [0.05425142]
    76 0.00042222455 [0.9767084] [0.05294728]
    77 0.00040216724 [0.97726834] [0.05167446]
    78 0.00038306369 [0.9778148] [0.05043224]
    79 0.0003648663 [0.9783481] [0.04921987]
    80 0.0003475372 [0.97886854] [0.04803665]
    81 0.0003310284 [0.97937655] [0.04688191]
    82 0.0003153045 [0.97987235] [0.0457549]
    83 0.0003003261 [0.98035616] [0.04465497]
    84 0.0002860618 [0.9808284] [0.04358151]
    85 0.000272472 [0.98128927] [0.04253384]
    86 0.00025953216 [0.9817391] [0.04151137]
    87 0.00024720284 [0.98217803] [0.04051345]
    88 0.00023545936 [0.9826065] [0.03953955]
    89 0.00022427457 [0.9830246] [0.03858905]
    90 0.00021362239 [0.9834327] [0.03766141]
    91 0.0002034764 [0.983831] [0.03675606]
    92 0.00019381085 [0.9842196] [0.03587244]
    93 0.00018460276 [0.984599] [0.03501011]
    94 0.0001758362 [0.98496926] [0.0341685]
    95 0.00016748202 [0.9853305] [0.03334709]
    96 0.00015952722 [0.9856832] [0.03254547]
    97 0.00015194919 [0.9860273] [0.03176309]
    98 0.00014473282 [0.98636323] [0.03099954]
    99 0.0001378571 [0.98669106] [0.03025434]
    


```python
print("X: 5, Y:",sess.run(hypothesis, feed_dict={X: 5}))
print("X: 2.5, Y:", sess.run(hypothesis, feed_dict={X: 2.5}))
sess.close()
```

    X: 5, Y: [4.96371]
    X: 2.5, Y: [2.496982]
    
